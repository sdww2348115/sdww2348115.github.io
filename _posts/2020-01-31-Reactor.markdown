---
layout: post
title:  "从BIO到Reactor"
date:   2020-01-31 22:45:00 +0800
categories: netty network nginx nio 
permalink: /io/Reactor
description: 从BIO到Reactor
---

## 传统BIO网络通信模型
apache与Tomcat等Web服务器为了提升充分利用服务器资源，使用多线程模型来并发处理多网络连接的情况：当client与服务器建立连接后，服务器就在内部建立一个线程/进程专门用于处理该连接上的数据。子线程/进程使用阻塞读写的方式操作网络IO，当client端没有新数据到来的时候，子线程/进程将进入阻塞状态，让出CPU资源供其他工作线程/进程使用。

![BIO](../resources/img/BIO.png)

传统BIO模型最大的问题在于高并发环境下的表现并不乐观，一个典型的例子便是C10K问题：当单台服务器接受的连接数上10K后，其对应的线程/进程数也达到10k之多。对于CPU而言，随着进程/线程数量的增多，对其进行调度的开销也就越大，用于业务处理的时间片相应减少；同时，大量进程/线程的创建和销毁同样会消耗大量CPU运算资源。大量的线程/进程还会不断消耗内存资源，单台服务器的内存往往也很难抗住如此大的并发量。

## NIO模型
根据上面的分析，传统BIO最大的问题和瓶颈在于`连接与进程/线程之间的一一绑定关系`。只要打破它们之间的一一对应关系，就能突破BIO模型的并发上限。如下图所示，多个Client所创建connection都被放在一个pool中，worker线程/进程将使用非阻塞的方式对其进行读/写操作。

![NIO](../resources/img/NIO.png)

首先不考虑多线程，我们将模型简化为单worker线程模型。让一个worker线程对应多个connection的关键在于非阻塞调用connection的读/写，如果connection能够读取到数据，则根据读取到的数据进行下一步操作；如果读取的数据长度为0，说明连接没有数据到来，继续对后面的connection进行轮询操作。

``` c
//设置socket的fd为非阻塞模式
socket = fcntl(s,F_SETFL,fcntl(s,F_GETFL,0)|O_NONBLOCK)
......
for (socket in connectionPool) {
    if (recv(client, buffer, 1024, 0) > 0) {
        //对读取到的数据进行业务操作
    } else {
        //do nothing,继续下一个操作
    }
}
```

根据上面的伪代码可以看出，每一次完整的循环，程序将调用N次recv()函数，事实上每一次recv()的调用开销是不小的。原因在于`当程序调用recv()时，程序将从用户态切换至系统态，将socket buffer中的数据拷贝至预留好的buffer中`。拷贝完成后，程序将回到用户态继续运行。

![data-copy](../resources/img/data-copy.gif)

> Tips：[程序在用户态与系统态之间相互切换时，涉及到栈的切换、上下文保存、额外的检验等操作，往往开销较大](https://segmentfault.com/q/1010000000522752)

![context-switch](../resources/img/context-switch.gif)

当connectionPool中涉及到大量socket时，每次循环光查看socket是否有数据到来就将耗费大量的时间，由于io操作天生的稀疏性，connectionPool中可能只有几个连接有数据到来，大量的时间被消耗在无意义的查询上了。因此，通常在进行IO编程时并不会直接通过recv()等方法来遍历所有socket fd，在实际的使用中，采用更多的是以下几种方式：

* select:同时将一大批需要监听的socket注册至操作系统中，每次轮询时可以获取一个数组，通过检查数组中每一个值便可知道此connection是否有数据到来（可读）。此方法将原来的N次用户/系统态切换操作通过批量的方式缩减为1次，大大提高了整个系统的效率。
* poll:核心原理与select差不多，主要针对select的易用性方面进行了优化，同时突破了select最大fd数量为1024的限制，可视为是select的增强版系统调用。但是由于大量fd在用户态与系统态之间切换、在fd数量较多的情况下其性能表现仍然不佳。
* epoll:epoll是当前linux环境下最有效的nio通信模式，它将多个fd就绪事件的扫描由`批量`的形式变为了`回调`，从易用性以及性能等多个方面对select/poll进行了增强。

> 1. Linux内核中epoll使用红黑树代替数组，从而突破了select 1024个fd的限制，同时将wait与modify两个语义通过不同方法进行解耦，修改需要watch的fd集合时可直接进行增量修改，不再需要全量替换

> 2. 在Linux系统内核态唤醒wait在epoll中的progress时，通过引入队列的方式，避免了操作系统对所有watch fd进行全量扫描，使得epoll收集fd状态时由O(N)的复杂度变为了O(1)

> 3. 借助于边缘触发机制，epoll_wait只会在fd有新数据到来时才会返回，更加贴合大量connection中实际数据较少的情况。

更多关于select/poll/epoll的区别与修改请见:[大话 Select、Poll、Epoll](https://cloud.tencent.com/developer/article/1005481)

## Reactor模型

